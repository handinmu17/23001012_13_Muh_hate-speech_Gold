{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e2651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES FOR REGEX, PANDAS, NUMPY, SQLITE3, MATPLOTLIB, SEABORN, AND WARNINGS (TO IGNORE VISUALIZATION RESULT WARNING\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# IMPORT LIBRARY FOR FLASK AND SWAGGER\n",
    "from flask import Flask, jsonify, request\n",
    "from flasgger import Swagger, LazyString, LazyJSONEncoder\n",
    "from flasgger import swag_from\n",
    "\n",
    "# DEFAULT FLASK AND SWAGGER DEFAULT SETTING\n",
    "app = Flask(__name__)\n",
    "app.json_encoder = LazyJSONEncoder\n",
    "swagger_template = dict(\n",
    "info = {\n",
    "    'title': LazyString(lambda: 'API Documentation for Data Processing and Modeling'),\n",
    "    'version': LazyString(lambda: '1.0.0'),\n",
    "    'description': LazyString(lambda: 'Dokumentasi API untuk Data Processing dan Modeling'),\n",
    "    },\n",
    "    host = LazyString(lambda: request.host)\n",
    ")\n",
    "swagger_config = {\n",
    "    \"headers\": [],\n",
    "    \"specs\": [\n",
    "        {\n",
    "            \"endpoint\": 'docs',\n",
    "            \"route\": '/docs.json',\n",
    "        }\n",
    "    ],\n",
    "    \"static_url_path\": \"/flasgger_static\",\n",
    "    \"swagger_ui\": True,\n",
    "    \"specs_route\": \"/docs/\"\n",
    "}\n",
    "swagger = Swagger(app, template=swagger_template,             \n",
    "                  config=swagger_config)\n",
    "\n",
    "# IMPORT ABUSIVE.CSV AND NEW_KAMUSALAY.CSV TO PANDAS DATAFRAME (EACH)\n",
    "df_abusive = pd.read_csv('abusive.csv')\n",
    "df_kamusalay = pd.read_csv('new_kamusalay.csv', encoding='latin-1', header=None)\n",
    "df_kamusalay.columns=[\"tidak baku\",\"baku\"]\n",
    "\n",
    "# DEFINE ENDPOINTS: BASIC GET\n",
    "@swag_from(r\"/Users/dyanachusnulittajatnika/Python Learning/Gold Challenge/hello_world.yml\", methods=['GET'])\n",
    "@app.route('/', methods=['GET'])\n",
    "def hello_world():\n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Menyapa Hello World\",\n",
    "        'data': \"Hello World\",\n",
    "    }\n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "# DEFINE ENDPOINTS: POST FOR TEXT PROCESSING FROM TEXT INPUT\n",
    "@swag_from(r\"/Users/dyanachusnulittajatnika/Python Learning/Gold Challenge/text_processing.yml\", methods=['POST'])\n",
    "@app.route('/text-processing', methods=['POST'])\n",
    "def text_processing():\n",
    "    \n",
    "    text = request.form.get('text')\n",
    "    \n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Teks yang sudah diproses\",\n",
    "        'data': re.sub(r'[^a-zA-Z0-9]',' ', text)\n",
    "    }\n",
    "    \n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "# DEFINE ENDPOINTS: POST FOR TEXT PROCESSING FROM FILE\n",
    "@swag_from(r\"/Users/dyanachusnulittajatnika/Python Learning/Gold Challenge/text_processing_file.yml\", methods=['POST'])\n",
    "@app.route('/text-processing-file', methods=['POST'])\n",
    "def text_processing_file():\n",
    "    global post_df\n",
    "    \n",
    "    # USING REQUEST TO GET FILE THAT HAS BEEN POSTED FROM API ENDPOINT\n",
    "    file = request.files.get('file')\n",
    "    \n",
    "    # IMPORT FILE OBJECT INTO PANDAS DATAFRAME (YOU CAN SPECIFY NUMBER OF ROWS IMPORTED USING PARAMETER nrows=(integer value) )\n",
    "    post_df = pd.read_csv(file, encoding='latin-1')\n",
    "    \n",
    "    # SET THE TWEET COLUMN ONLY FOR THE DATAFRAME\n",
    "    post_df = post_df[['Tweet']]\n",
    "    \n",
    "    # DROP DUPLICATED TWEETS\n",
    "    post_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # CREATE NEW NUMBER OF CHARACTERS (NO_CHAR) COLUMN THAT CONSISTS OF LENGTH OF TWEET CHARACTERS\n",
    "    post_df['no_char'] = post_df['Tweet'].apply(len)\n",
    "    \n",
    "    # CREATE NEW NUMBER OF WORDS (NO_WORDS) COLUMN THAT CONSISTS OF NUMBER OF WORDS OF EACH TWEET\n",
    "    post_df['no_words'] = post_df['Tweet'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    # CREATE A FUNCTION TO CLEAN DATA FROM ANY NON ALPHA-NUMERIC (AND NON-SPACE) CHARACTERS, AND STRIP IT FROM LEADING/TRAILING SPACES\n",
    "    def tweet_cleansing(x):\n",
    "        tweet = x\n",
    "        cleaned_tweet = re.sub(r'[^a-zA-Z0-9 ]','',tweet).strip()\n",
    "        return cleaned_tweet\n",
    "    \n",
    "    # APPLY THE TWEET_CLEANSING FUNCTION ON TWEET COLUMN, AND CREATE A NEW CLEANED_TWEET COLUMN\n",
    "    post_df['cleaned_tweet'] = post_df['Tweet'].apply(lambda x: tweet_cleansing(x))\n",
    "    \n",
    "    # CREATE NEW NO_CHAR, AND NO_WORDS COLUMNS BASED ON CLEANED_TWEET COLUMN\n",
    "    post_df['no_char_2'] = post_df['cleaned_tweet'].apply(len)\n",
    "    post_df['no_words_2'] = post_df['cleaned_tweet'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    # CREATE A FUNCTION TO COUNT NUMBER OF ABUSIVE WORDS FOUND IN A CLEANED TWEET\n",
    "    def count_abusive(x):\n",
    "        cleaned_tweet = x\n",
    "        matched_list = []\n",
    "        for i in range(len(df_abusive)):\n",
    "            for j in x.split():\n",
    "                word = df_abusive['ABUSIVE'].iloc[i]\n",
    "                if word==j.lower():\n",
    "                    matched_list.append(word)\n",
    "        return len(matched_list)\n",
    "    \n",
    "    # APPLY THE FUNCTION TO COUNT ABUSIVE WORDS, AND CREATE A NEW COLUMN BASED OFF OF IT\n",
    "    post_df['estimated_no_abs_words'] = post_df['cleaned_tweet'].apply(lambda x: count_abusive(x))\n",
    "\n",
    "    # CREATE A DICTIONARY TO STORE THE COUNT OF EACH ABUSIVE WORD.\n",
    "    abusive_word_count = {}\n",
    "\n",
    "    # ITERATE THROUGH EACH ABUSIVE WORD\n",
    "    for abusive_word in df_abusive['ABUSIVE']:\n",
    "        # Count the occurrences of the abusive word in all tweets and store it in the dictionary\n",
    "        abusive_word_count[abusive_word] = post_df['Tweet'].str.count(abusive_word, flags=re.IGNORECASE).sum()\n",
    "\n",
    "    # CONNECT / CREATE NEW DATABASE AND CREATE NEW TABLE CONSISTING LISTED TABLES\n",
    "    conn = sqlite3.connect('database_project.db')\n",
    "    q_create_table = \"\"\"\n",
    "    create table if not exists post_df (Tweet varchar(255), no_char int, no_words int, cleaned_tweet varchar(255), no_char_2 int, no_words_2 int);\n",
    "    \"\"\"\n",
    "    conn.execute(q_create_table)\n",
    "    conn.commit()\n",
    "    \n",
    "    # CHECK WHETHER TABLE ALREADY HAS DATA IN IT (TABLE HAS ROWS OF DATA IN IT)\n",
    "    cursor = conn.execute(\"select count(*) from post_df\")\n",
    "    num_rows = cursor.fetchall()\n",
    "    num_rows = num_rows[0][0]\n",
    "    \n",
    "    #  DO DATA INSERTIONS IF TABLE HAS NO DATA IN IT    \n",
    "    if num_rows == 0:\n",
    "    # DO ITERATIONS TO INSERT DATA (EACH ROW) FROM FINAL DATAFRAME (POST_DF)\n",
    "        for i in range(len(post_df)):\n",
    "            tweet = post_df['Tweet'].iloc[i]\n",
    "            no_char = int(post_df['no_char'].iloc[i])\n",
    "            no_words = int(post_df['no_words'].iloc[i])\n",
    "            cleaned_tweet = post_df['cleaned_tweet'].iloc[i]\n",
    "            no_char_2 = int(post_df['no_char_2'].iloc[i])\n",
    "            no_words_2 = int(post_df['no_words_2'].iloc[i])\n",
    "    \n",
    "            q_insertion = \"insert into post_df (Tweet, no_char, no_words, cleaned_tweet, no_char_2, no_words_2) values (?,?,?,?,?,?)\"\n",
    "            conn.execute(q_insertion,(tweet,no_char,no_words,cleaned_tweet,no_char_2,no_words_2))\n",
    "            conn.commit()    \n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    # CONNECT / CREATE NEW DATABASE AND CREATE NEW TABLE FOR LIST OF ABUSIVE WORDS AND ITS COUNTING NUMBER\n",
    "    conn = sqlite3.connect('database_abusive_word.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''CREATE TABLE IF NOT EXISTS abusive_word_count (\n",
    "                        abusive_word TEXT PRIMARY KEY,\n",
    "                        count INTEGER\n",
    "                    )''')\n",
    "    # Insert the abusive words and their counts into the table\n",
    "    for abusive_word, count in abusive_word_count.items():\n",
    "        cursor.execute('INSERT OR REPLACE INTO abusive_word_count (abusive_word, count) VALUES (?, ?)', (abusive_word, count))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    # VISUALIZE THE NUMBER OF ABUSIVE WORDS USING BARPLOT (COUNTPLOT)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    countplot = sns.countplot(data=post_df, x=\"estimated_no_abs_words\")\n",
    "    for p in countplot.patches:\n",
    "        countplot.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() / 2., p.get_height()),  ha = 'center'\n",
    "                            , va = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
    "    \n",
    "    plt.title('Count of Estimated Number of Abusive Words')\n",
    "    plt.xlabel('Estimated Number of Abusive Words')\n",
    "    plt.savefig('new_countplot.jpeg')\n",
    "    \n",
    "    \n",
    "    # VISUALIZE THE NUMBER OF WORDS USING BOXPLOT\n",
    "    plt.figure(figsize=(20,4))\n",
    "    boxplot = sns.boxplot(data=post_df, x=\"no_words_2\")\n",
    "\n",
    "    plt.title('Number of Words Boxplot (after tweet cleansing)')\n",
    "    plt.xlabel('')\n",
    "    plt.savefig('new_boxplot.jpeg')\n",
    "\n",
    "    # VISUALIZE THE NUMBER OF WORDS USING WORDCLOUD\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(abusive_word_count)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.savefig('new_wordcloud.jpeg')\n",
    "    \n",
    "    # OUTPUT THE RESULT IN JSON FORMAT\n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Teks yang sudah diproses\",\n",
    "        'data': list(post_df['cleaned_tweet'])\n",
    "    }\n",
    "    \n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ch_venv",
   "language": "python",
   "name": "ch_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
